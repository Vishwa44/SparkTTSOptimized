{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Qwen2ForCausalLM, Qwen2Config\n",
    "from transformers.models.qwen2.modeling_qwen2 import ALL_ATTENTION_FUNCTIONS, eager_attention_forward\n",
    "\n",
    "import torch\n",
    "from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
    "from sparktts.utils.token_parser import LEVELS_MAP, GENDER_MAP, TASK_TOKEN_MAP\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "from sparktts.utils.file import load_config\n",
    "from torch.profiler import profile, record_function, ProfilerActivity, tensorboard_trace_handler\n",
    "import torch.cuda.nvtx as nvtx\n",
    "import time\n",
    "from transformers import generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\npython -m cli.inference     --text \"text to synthesis.\"     --device 0     --save_dir output     --model_dir /home/vishwa/small_projects/pretrained_model     --prompt_speech_path example/prompt_audio.wav\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "python -m cli.inference \\\n",
    "    --text \"text to synthesis.\" \\\n",
    "    --device 0 \\\n",
    "    --save_dir output \\\n",
    "    --model_dir /home/vishwa/small_projects/pretrained_model \\\n",
    "    --prompt_speech_path example/prompt_audio.wav\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/vishwa/small_projects/pretrained_model\"\n",
    "device = torch.device(\"cuda:0\")\n",
    "text = \"Hi! How are you?\"\n",
    "prompt_speech_path = \"example/prompt_audio.wav\"\n",
    "temperature = 0.8\n",
    "top_k = 50\n",
    "top_p = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishwa/miniconda3/envs/oumi/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing tensor: mel_transformer.spectrogram.window\n",
      "Missing tensor: mel_transformer.mel_scale.fb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(166000, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=166000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"{model_dir}/LLM\")\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"{model_dir}/LLM\", torch_dtype=\"float32\", _attn_implementation=\"sdpa\")\n",
    "audio_tokenizer = BiCodecTokenizer(model_dir, device=device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prompt(\n",
    "    text: str,\n",
    "    prompt_speech_path: Path,\n",
    "    prompt_text: str = None,\n",
    ") -> Tuple[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Process input for voice cloning.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text input to be converted to speech.\n",
    "        prompt_speech_path (Path): Path to the audio file used as a prompt.\n",
    "        prompt_text (str, optional): Transcript of the prompt audio.\n",
    "\n",
    "    Return:\n",
    "        Tuple[str, torch.Tensor]: Input prompt; global tokens\n",
    "    \"\"\"\n",
    "\n",
    "    global_token_ids, semantic_token_ids = audio_tokenizer.tokenize(\n",
    "        prompt_speech_path\n",
    "    )\n",
    "    global_tokens = \"\".join(\n",
    "        [f\"<|bicodec_global_{i}|>\" for i in global_token_ids.squeeze()]\n",
    "    )\n",
    "\n",
    "    # Prepare the input tokens for the model\n",
    "    if prompt_text is not None:\n",
    "        semantic_tokens = \"\".join(\n",
    "            [f\"<|bicodec_semantic_{i}|>\" for i in semantic_token_ids.squeeze()]\n",
    "        )\n",
    "        inputs = [\n",
    "            TASK_TOKEN_MAP[\"tts\"],\n",
    "            \"<|start_content|>\",\n",
    "            prompt_text,\n",
    "            text,\n",
    "            \"<|end_content|>\",\n",
    "            \"<|start_global_token|>\",\n",
    "            global_tokens,\n",
    "            \"<|end_global_token|>\",\n",
    "            \"<|start_semantic_token|>\",\n",
    "            semantic_tokens,\n",
    "        ]\n",
    "    else:\n",
    "        inputs = [\n",
    "            TASK_TOKEN_MAP[\"tts\"],\n",
    "            \"<|start_content|>\",\n",
    "            text,\n",
    "            \"<|end_content|>\",\n",
    "            \"<|start_global_token|>\",\n",
    "            global_tokens,\n",
    "            \"<|end_global_token|>\",\n",
    "        ]\n",
    "\n",
    "    inputs = \"\".join(inputs)\n",
    "\n",
    "    return inputs, global_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, global_token_ids = process_prompt(text, prompt_speech_path)\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[165137, 165146,  13048,      0,   2585,    525,    498,     30, 165152,\n",
       "         165150, 155028, 154032, 153256, 155290, 151959, 155221, 152863, 154311,\n",
       "         155062, 155443, 154124, 154774, 152682, 155525, 154843, 155079, 153392,\n",
       "         152226, 152761, 154798, 155312, 154843, 154432, 151738, 154083, 153503,\n",
       "         155310, 154003, 154092, 153809, 151971, 154464, 165156]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     with profile(\n",
    "#             activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "#             schedule=torch.profiler.schedule(wait=1, warmup=1, active=1, repeat=3),\n",
    "#             record_shapes=True,\n",
    "#             profile_memory=True,\n",
    "#             with_stack=True,\n",
    "#             with_flops=True,\n",
    "#             with_modules=True,\n",
    "#             on_trace_ready=tensorboard_trace_handler('./profiler/forwardpass_sdpa')\n",
    "#         ) as prof:\n",
    "#             for _ in range(10):\n",
    "#                 ip = torch.randint(low = 0, high=166000,size=(1,1000)).cuda()\n",
    "#                 generated_ids = model(ip)\n",
    "#                 prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model(model_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = generated_ids.logits[0, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([166000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if temperature > 0:\n",
    "        temp_logits = next_token_logits / temperature\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "probs = torch.softmax(temp_logits, dim=-1)\n",
    "\n",
    "sorted_probs, sorted_indices = torch.sort(probs, descending=False)\n",
    "\n",
    "if top_k > 0:\n",
    "    # Keep only top-k tokens\n",
    "    sorted_probs = sorted_probs[:top_k]\n",
    "    sorted_indices = sorted_indices[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def fused_temperature_softmax_kernel(\n",
    "    logits_ptr, out_ptr, \n",
    "    temp, top_k, top_p,\n",
    "    n_vocab, BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    \"\"\"\n",
    "    Fused kernel for temperature scaling, top-k and top-p sampling.\n",
    "    \n",
    "    Args:\n",
    "        logits_ptr: Pointer to logits tensor\n",
    "        out_ptr: Pointer to output tensor\n",
    "        temp: Temperature value\n",
    "        top_k: k value for top-k sampling\n",
    "        top_p: p value for top-p sampling\n",
    "        n_vocab: Size of vocabulary\n",
    "    \"\"\"\n",
    "    # Get program ID\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # Compute offsets\n",
    "    offset_base = pid * n_vocab\n",
    "    \n",
    "    # Load logits\n",
    "    mask = tl.arange(0, BLOCK_SIZE) < n_vocab\n",
    "    logits = tl.load(logits_ptr + offset_base + tl.arange(0, BLOCK_SIZE), mask=mask, other=-float('inf'))\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    logits_scaled = logits / temp\n",
    "    \n",
    "    row_minus_max = logits_scaled - tl.max(logits_scaled, axis=0)\n",
    "        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "    numerator = tl.exp(row_minus_max)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    softmax_output = numerator / denominator\n",
    "    \n",
    "    \n",
    "    # Store result\n",
    "    tl.store(out_ptr + offset_base + tl.arange(0, BLOCK_SIZE), softmax_output, mask=mask)\n",
    "\n",
    "@triton.jit\n",
    "def fused_topk_topp_kernel(\n",
    "    sorted_probs_ptr, out_ptr, \n",
    "    top_k, top_p,\n",
    "    n_vocab, BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    \"\"\"\n",
    "    Fused kernel for top-k and top-p sampling on already sorted probabilities.\n",
    "    \n",
    "    Args:\n",
    "        sorted_probs_ptr: Pointer to sorted probabilities tensor\n",
    "        out_ptr: Pointer to output tensor\n",
    "        top_k: k value for top-k sampling\n",
    "        top_p: p value for top-p sampling\n",
    "        n_vocab: Size of vocabulary\n",
    "    \"\"\"\n",
    "    # Get program ID\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # Compute offsets\n",
    "    offset_base = pid * n_vocab\n",
    "    \n",
    "    # Load sorted probs\n",
    "    mask = tl.arange(0, BLOCK_SIZE) < n_vocab\n",
    "    offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    sorted_probs = tl.load(sorted_probs_ptr + offset_base + offsets, mask=mask, other=0.0)\n",
    "    \n",
    "    # Initialize result mask (start with all tokens)\n",
    "    result_mask = mask\n",
    "    \n",
    "    # Apply top-k filtering if specified\n",
    "    if top_k > 0:\n",
    "        k_val = tl.minimum(n_vocab, top_k)\n",
    "        k_mask = offsets < k_val\n",
    "        result_mask = mask & k_mask\n",
    "    \n",
    "    # # Apply top-p filtering if specified\n",
    "    # if top_p > 0 and top_p < 1.0:\n",
    "    #     cumulative_probs = tl.cumsum(sorted_probs)\n",
    "    #     # Keep tokens whose cumulative probability is <= top_p\n",
    "    #     p_mask = cumulative_probs <= top_p\n",
    "    #     # Always include at least the first token\n",
    "    #     p_mask = p_mask | (offsets == 0)\n",
    "    #     result_mask = result_mask & p_mask\n",
    "    \n",
    "    # Apply the combined mask to the probabilities\n",
    "    filtered_probs = tl.where(result_mask, sorted_probs, 0.0)\n",
    "    \n",
    "    # Renormalize the remaining probabilities (optional)\n",
    "    # sum_probs = tl.sum(filtered_probs, axis=0)\n",
    "    # filtered_probs = tl.where(sum_probs > 0.0, filtered_probs / sum_probs, filtered_probs)\n",
    "    \n",
    "    # Store result\n",
    "    tl.store(out_ptr + offset_base + offsets, filtered_probs, mask=mask)\n",
    "\n",
    "\n",
    "# Wrapper function to call the kernel\n",
    "def sample_next_token_triton(logits, temperature=1.0, top_k=0, top_p=1.0):\n",
    "    vocab_size = logits.shape[-1]\n",
    "    output = torch.empty_like(logits)\n",
    "    print(logits.shape)\n",
    "    # Configure grid\n",
    "    grid = (1,)\n",
    "    # Launch kernel\n",
    "    fused_temperature_softmax_kernel[grid](\n",
    "        logits, output, \n",
    "        temperature, top_k, top_p,\n",
    "        vocab_size, triton.next_power_of_2(vocab_size)\n",
    "    )\n",
    "    print(\"softmax done\")\n",
    "    sorted_probs, sorted_indices = torch.sort(output, descending=True)\n",
    "    # output = torch.empty_like(logits)\n",
    "    # fused_topk_topp_kernel[grid](\n",
    "    #     sorted_probs, output, \n",
    "    #     temperature, top_k, top_p,\n",
    "    #     vocab_size, triton.next_power_of_2(vocab_size)\n",
    "    # )\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([166000])\n"
     ]
    }
   ],
   "source": [
    "triton_probs = sample_next_token_triton(logits=next_token_logits, temperature=temperature, top_k=top_k, top_p=top_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
